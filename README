	An average of 2.5 million car crashes occur annually of which 1.3 million people die. Drunk and reckless drivers are the cause of around 10 thousand deaths each year in the United States alone.  With the implementation of autonomous cars, thousands of lives can be spared. Large corporations such as Tesla and Google, have worked around the idea of self-driving cars for a long time with the use of machine learning concepts. Convolutional neural networks (CNN) are often used for visual tasks and can be used for the identification of various road signs. So, how effective are different CNN architectures for classifying a road sign as a traffic light, stop sign, speed limit or crosswalk sign? 
	Convolutional neural network (CNN) is a method of machine learning used for image classification and visual tasks. CNNs are based on the visual cortex of the brain and several recent advancements have increased the use of CNNs in many upcoming projects. CNNs consist of several convolutional layers which an image is passed through to get an output. Convolution layers extract features from the image. Training data must be used for CNNs to adjust the weights and biases which contribute to the final accuracy. A convolutional neural network can be used for this project because the goal of the program is to classify road signs which is a visual task and can be achieved using CNN. CNN can be implemented into a code using several methods, however one of the most common is keras. Keras was developed by Google and is a deep learning API that can be used to facilitate the use of CNNs. Keras was written in python and can currently only be used in python code.  
	The dataset used for this code was taken from Kaggle. The name of the dataset is “Road Sign Detection” and was created by the user, LARXEL.
Correctly Classified Images:

Incorrectly Classified Images:

Training and validation accuracy across 20 epochs:
Epoch 1/20
5/5 [==============================] - 1s 160ms/step - loss: 1.0893 - accuracy: 0.7064 - val_loss: 0.8169 - val_accuracy: 0.7614
Epoch 2/20
5/5 [==============================] - 0s 82ms/step - loss: 0.8192 - accuracy: 0.7471 - val_loss: 0.7882 - val_accuracy: 0.7614
Epoch 3/20
5/5 [==============================] - 0s 65ms/step - loss: 0.7979 - accuracy: 0.7455 - val_loss: 0.7993 - val_accuracy: 0.7614
Epoch 4/20
5/5 [==============================] - 0s 68ms/step - loss: 0.7749 - accuracy: 0.7504 - val_loss: 0.7773 - val_accuracy: 0.7500
Epoch 5/20
5/5 [==============================] - 0s 56ms/step - loss: 0.7271 - accuracy: 0.7537 - val_loss: 0.7499 - val_accuracy: 0.7500
Epoch 6/20
5/5 [==============================] - 0s 59ms/step - loss: 0.7005 - accuracy: 0.7651 - val_loss: 0.7302 - val_accuracy: 0.7500
Epoch 7/20
5/5 [==============================] - 0s 60ms/step - loss: 0.6713 - accuracy: 0.7798 - val_loss: 0.7073 - val_accuracy: 0.7386
Epoch 8/20
5/5 [==============================] - 0s 58ms/step - loss: 0.6449 - accuracy: 0.7847 - val_loss: 0.7069 - val_accuracy: 0.7386
Epoch 9/20
5/5 [==============================] - 0s 67ms/step - loss: 0.6118 - accuracy: 0.7977 - val_loss: 0.8284 - val_accuracy: 0.7273
Epoch 10/20
5/5 [==============================] - 0s 61ms/step - loss: 0.6605 - accuracy: 0.8042 - val_loss: 0.7231 - val_accuracy: 0.7500
Epoch 11/20
5/5 [==============================] - 0s 60ms/step - loss: 0.5858 - accuracy: 0.8271 - val_loss: 0.6947 - val_accuracy: 0.8068
Epoch 12/20
5/5 [==============================] - 0s 60ms/step - loss: 0.5301 - accuracy: 0.8303 - val_loss: 0.6853 - val_accuracy: 0.7955
Epoch 13/20
5/5 [==============================] - 0s 62ms/step - loss: 0.4889 - accuracy: 0.8369 - val_loss: 0.6999 - val_accuracy: 0.8068
Epoch 14/20
5/5 [==============================] - 0s 60ms/step - loss: 0.4638 - accuracy: 0.8548 - val_loss: 0.6556 - val_accuracy: 0.8068
Epoch 15/20
5/5 [==============================] - 0s 54ms/step - loss: 0.4377 - accuracy: 0.8777 - val_loss: 0.7396 - val_accuracy: 0.8068
Epoch 16/20
5/5 [==============================] - 0s 55ms/step - loss: 0.3965 - accuracy: 0.8842 - val_loss: 0.6655 - val_accuracy: 0.8182
Epoch 17/20
5/5 [==============================] - 0s 54ms/step - loss: 0.3697 - accuracy: 0.8825 - val_loss: 0.7157 - val_accuracy: 0.8068
Epoch 18/20
5/5 [==============================] - 0s 57ms/step - loss: 0.3290 - accuracy: 0.8940 - val_loss: 0.6621 - val_accuracy: 0.8068
Epoch 19/20
5/5 [==============================] - 0s 56ms/step - loss: 0.2924 - accuracy: 0.9119 - val_loss: 0.7267 - val_accuracy: 0.8182
Epoch 20/20
5/5 [==============================] - 0s 57ms/step - loss: 0.2583 - accuracy: 0.9217 - val_loss: 0.7977 - val_accuracy: 0.7955
	Overall, using a CNN model for this project produced satisfactory results with an accuracy of around 80%. The original code had an accuracy of 72%, however adding an additional convolutional increased the accuracy of the code. CNNs worked well in this context and the results were as imagined. The code can be improved by adding more layers; however, a different method of machine learning could produce an increased accuracy score. Another way the accuracy of the code can be improved is by using a larger dataset. A larger dataset would increase the amount of training data which can lead to improved results. In the end, the code can be improved using various methods, however using CNN is a good approach for adequate results. 

